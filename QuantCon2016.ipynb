{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Matrix Regularization with AutoEncoders\n",
    "\n",
    "In this project, we will demonstrate how to implement a simple AutoEncoder in TensorFlow and integrate it with Zipline to test the sample covariance matrix regularization with such an approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's get some data.\n",
    "\n",
    "Our sample universe will constitute from the equities of 3 major tech companies, an energy producer, a multinational beverage corporation and ETF's for Gold and 20Y US Treasury Bond.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7 insts load\n",
    "\n",
    "\n",
    "universe = ['IBM', 'AAPL', 'MSFT', 'XOM', 'KO', 'GLD', 'TLT']\n",
    "start = '20060101'\n",
    "end = '20160228'\n",
    "\n",
    "data = load_bars_from_yahoo(stocks=universe, start=start, end=end)\n",
    "    \n",
    "data.loc[:, :, 'price'].plot(figsize=(15,10))\n",
    "plt.ylabel('price in $');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder Graph Builder\n",
    "\n",
    "Now define a stand-alone TensorFlow graph which will act like an autoencoder. We also defined our loss function in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A Simple multi-layer Autoencoder definition in TensorFlow\n",
    "# based on https://github.com/pkmital/tensorflow_tutorials/\n",
    "\n",
    "def autoencoder(dimensions=[1], denoise_type=0):\n",
    "    \n",
    "    # %% input to the network\n",
    "    x = tf.placeholder(tf.float32, [None, dimensions[0]], name='x')\n",
    "    current_input = x\n",
    "\n",
    "    # %% denoising component\n",
    "    corrupt_prob = tf.placeholder(tf.float32, [1])\n",
    "    if denoise_type==1:\n",
    "        current_input = tf.mul(x, tf.cast(tf.random_uniform(shape=tf.shape(x),minval=0,\n",
    "                    maxval=2, dtype=tf.int32), tf.float32)) * corrupt_prob + x * (1 - corrupt_prob)\n",
    "    if denoise_type==2:\n",
    "        current_input = occlude_input(x,corrupt_prob)\n",
    "        \n",
    "\n",
    "    # first build the encoder\n",
    "    encoder = []\n",
    "    for layer_i, n_output in enumerate(dimensions[1:]):\n",
    "        n_input = int(current_input.get_shape()[1])\n",
    "        W = tf.Variable( tf.random_uniform([n_input, n_output], -1.0 / math.sqrt(n_input),\n",
    "                        1.0 / math.sqrt(n_input)), name=\"EncWeights-%d\" % layer_i)\n",
    "        b = tf.Variable(tf.zeros([n_output]), name=\"EncBias-%d\" % layer_i)\n",
    "        encoder.append(W)\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "\n",
    "        # Set the num of inputs for the next layer\n",
    "        current_input = output\n",
    "\n",
    "    # code layer \n",
    "    z = current_input\n",
    "    encoder.reverse()\n",
    "\n",
    "    # construct the Decoder layers using the same weights\n",
    "    for layer_i, n_output in enumerate(dimensions[:-1][::-1]):\n",
    "        W = tf.transpose(encoder[layer_i], name=\"DecWeights-%d\" % layer_i)\n",
    "        b = tf.Variable(tf.zeros([n_output]), name=\"DecBias-%d\" % layer_i)\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        current_input = output\n",
    "\n",
    "    # this is the reconstruction through the network\n",
    "    y = current_input\n",
    "    \n",
    "    # cost function is the average absolute value of pixel-wise differences\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.square(y - x)))\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "\n",
    "    return {'x': x, 'z': z, 'y': y, \n",
    "            'corrupt_prob': corrupt_prob, 'cost': cost, 'cost_summ':cost_summ}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and prediction method\n",
    "\n",
    "\n",
    "Initiation step:\n",
    "\n",
    "The following method operates in two separate regimes depending if a tensor-flow-context dictionary is provided;\n",
    "\n",
    "\n",
    "1. If not it will first use the above method to instantiate an autoencoder, embed it into a TF graph and session. It will also define and attach the optimization op to be used during training.\n",
    "\n",
    "2. If tensor-flow-context is provided, then it will use that context to further train an existing graph with new data in next step.\n",
    "\n",
    "\n",
    "Training and estimation step:\n",
    "\n",
    "Convert covariance matrices provided to correlation and a normalized volatility vector and train the network using those normalized data. Once training is done feed the covmat_target (the last observation of training if None) to the network to receive the regularized vector. Finally, convert this regularized vector of correlations and volatilities back to final covariance estimate. The manipulation of matrices and dividing the sample set into various batches is handled by the following utility class CovarianceDataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def DeepShrinkCovmat(covmats_train, tensorflowcontext=None, covmat_target=None, \n",
    "                     args_ae={'layer_dims' : [6,4], 'learn_rate':0.001, 'epochs':1, 'denoise_type':1, \n",
    "                              'batch_size':10, 'shuffle_batches':True, 'usevola':False} ):\n",
    "    \n",
    "    # Initializations\n",
    "    trdataset = CovarianceDataSet(covmats_train, shuffle=args_ae['shuffle_batches'])\n",
    "    \n",
    "    if covmat_target is None:\n",
    "        covmat_target = covmats_train[-1]   # If no covmat-to-shrink provided shrink the last observation \n",
    "    \n",
    "    N = covmat_target.shape[0]\n",
    "    nItems = N * (N-1) / 2 + N\n",
    "\n",
    "    if tensorflowcontext is None:\n",
    "        dims = [nItems]\n",
    "        dims.extend(args_ae['layer_dims'])\n",
    "        ae = autoencoder(dimensions=dims, denoise_type=args_ae['denoise_type'])\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(args_ae['learn_rate']).minimize(ae['cost'])\n",
    "        \n",
    "        # We create a TensorFlow session to use the graph\n",
    "        sess = tf.Session()\n",
    "        \n",
    "        # Define summary merger and log-writer for TensorBoard\n",
    "        merged = tf.merge_all_summaries()\n",
    "        writer = tf.train.SummaryWriter(\"./tb_logs\", sess.graph_def)\n",
    "                                       \n",
    "        # Initialize all TF variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    else:\n",
    "        sess = tensorflowcontext[\"sess\"]\n",
    "        ae = tensorflowcontext[\"ae\"]\n",
    "        optimizer = tensorflowcontext[\"optimizer\"]\n",
    "        merged = tensorflowcontext[\"merged\"]\n",
    "        writer = tensorflowcontext[\"writer\"]\n",
    "        \n",
    "    \n",
    "    vola_target = np.sqrt(np.diag(covmat_target))\n",
    "            \n",
    "    # Iterate through training set via mini-batches\n",
    "    last_cost = None\n",
    "    corr_prob = 0.0\n",
    "    if args_ae['denoise_type']<>0:\n",
    "        corr_prob = 1.0\n",
    "    \n",
    "    # This is where learning happens by iterating through the samples and running the optimizer\n",
    "    for step in xrange(0, args_ae['epochs'] * trdataset.num_examples, args_ae['batch_size']):\n",
    "            sess.run( optimizer, feed_dict={ae['x']: trdataset.next_batch(args_ae['batch_size']), \n",
    "                                           ae['corrupt_prob']:[corr_prob]} )\n",
    "    \n",
    "    target_norm = trdataset.get_normal_form(covmat_target)\n",
    "    \n",
    "    recons = sess.run( [ae['y'],ae['cost_summ']], feed_dict={ae['x']: target_norm, ae['corrupt_prob']:[0.0]} )\n",
    "    recon = recons[0][0]\n",
    "    cost_summ = recons[1]\n",
    "    writer.add_summary(cost_summ)\n",
    "    \n",
    "    cov_recon = np.diag(np.diag(covmat_target))\n",
    "    \n",
    "    try:     \n",
    "        recon_vola = None\n",
    "        if not args_ae['usevola']:\n",
    "            recon_vola = vola_target \n",
    "        \n",
    "        cov_recon = trdataset.reconstruct_covariance(recon,recon_vola)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print 'Warning: ' + str(e)\n",
    "        np.copyto(covmat_target, cov_recon)\n",
    "\n",
    "\n",
    "    return cov_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CovarianceDataSet(object):\n",
    "            \n",
    "    def __init__(self, cov_samples, shuffle=True):\n",
    "        assert len(cov_samples) > 1\n",
    "        assert len(cov_samples[0].shape) == 2\n",
    "        assert cov_samples[0].shape[0] == cov_samples[0].shape[1]\n",
    "\n",
    "        self._num_examples = len(cov_samples)\n",
    "        self._shuffle = shuffle\n",
    "\n",
    "        # Convert shape from [num examples, N, N]\n",
    "        # to [num examples, N*(N-1)/2 + N] which stands for \n",
    "        # normalized upper triangular elements\n",
    "        self._N = cov_samples[0].shape[0]\n",
    "        nItems = self._N * (self._N-1) / 2 + self._N\n",
    "\n",
    "        self._volas_array = np.asarray([np.sqrt(diag(cmat)) for cmat in cov_samples])\n",
    "        self._volas_range = np.asarray([(np.min(self._volas_array[:,n_i]), \n",
    "                                         np.max(self._volas_array[:,n_i])) \n",
    "                                            for n_i in range(self._N)])\n",
    "\n",
    "        self._volas_norm = np.asarray([ self.scalemin11(self._volas_array[:,n_i]) \n",
    "                                            for n_i in range(self._N) ])\n",
    "\n",
    "        self._trainingdata = np.asarray([ np.append(cov2corr(cmat)[np.triu_indices(self._N,k=1)], \n",
    "                                          self._volas_norm[:,idx]) \n",
    "                                        for idx, cmat in enumerate(cov_samples) ])  \n",
    "        \n",
    "        self._trainingdata[self._trainingdata < -1.0 + 1.0e-9] = -1.0 + 1.0e-9\n",
    "        self._trainingdata[self._trainingdata > 1.0 - 1.0e-9] = 1.0 - 1.0e-9\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def training_data(self):\n",
    "        return self._trainingdata\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def scalemin11(self, vec):       # Scale the vec to [-1, 1]\n",
    "        if vec.min() == vec.max(): return vec       # to avoid division by zero\n",
    "        return( 2 * (vec-vec.min()) / float(vec.max()-vec.min()) - 1 )\n",
    "\n",
    "    def scalemin11back(self, vec, rangev):\n",
    "        if rangev[0] == rangev[1]: return vec\n",
    "        return( (vec+1)/2 * float(rangev[1]-rangev[0]) + rangev[0] )\n",
    "\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            if self._shuffle:\n",
    "                # Shuffle the data\n",
    "                perm = np.arange(self._num_examples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._trainingdata = self._trainingdata[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "\n",
    "        return self._trainingdata[start:end]\n",
    "\n",
    "    def get_normal_form(self, target_mat):\n",
    "        vola_target = np.sqrt(np.diag(target_mat))\n",
    "        return np.asarray([np.append(cov2corr(target_mat)[np.triu_indices(self._N,k=1)], \n",
    "                np.asarray([ (vola_target[idx] - self._volas_range[idx][0])/\n",
    "                            (self._volas_range[idx][1]- self._volas_range[idx][0]) \n",
    "                            for idx in xrange(self._N) ]))])\n",
    "\n",
    "    \n",
    "    def reconstruct_covariance(self, recon, use_vola=None):\n",
    "        recon_corr = np.eye(self._N)   \n",
    "        recon_vola = np.asarray([ self.scalemin11back(recon[-self._N:][idx], rangev) \n",
    "                                 for idx, rangev in enumerate(self._volas_range) ])\n",
    "\n",
    "        tri_inds_u = np.triu_indices(self._N,k=1)\n",
    "        tri_inds_l = tri_inds_u[1], tri_inds_u[0]\n",
    "        recon_corr[tri_inds_u] = recon[:-self._N]\n",
    "        recon_corr[tri_inds_l] = recon[:-self._N]\n",
    "\n",
    "        # Ensure that the regularized correlation matrix will be positive semidefinite\n",
    "        recon_corr = nearcorr(recon_corr)\n",
    "        if use_vola is not None:\n",
    "            recon_vola = use_vola\n",
    "            \n",
    "        # Finally, reconstruct covariance matrix from correlations using the volatility estimates\n",
    "        return corr2cov(recon_corr, recon_vola)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the method we just defined with some real data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = data[:,-700:,'price']\n",
    "returns = dat.pct_change().dropna()\n",
    "datalen = len(returns)\n",
    "covlen = 250\n",
    "\n",
    "training_covs = [ np.asarray(returns[i:i+covlen].cov()) for i in range(datalen-covlen+1) ]\n",
    "cov_target = training_covs[-1]\n",
    "\n",
    "\n",
    "sim_args = { 'layer_dims':[64,32,4], 'learn_rate':0.001, 'epochs':2, 'denoise_type':1, \n",
    "             'batch_size':10, 'shuffle_batches':True, 'usevola':False } \n",
    "\n",
    "cov_recon = DeepShrinkCovmat(training_covs, args_ae = sim_args )\n",
    "\n",
    "ShowCovarianceMatrix(cov_target, universe, title='Sample Covariance')\n",
    "ShowCovarianceMatrix(cov_recon, universe, title='AutoEncoder Covariance')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest Method with Tensorflow and Zipline integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RunBacktest(datatorun, rebalfreq=20, covlen=250, trainlen=100, corrupted_inp=.0, dump_covs = False,\n",
    "                use_autoencoder=False, args_ae={ 'layer_dims':[1], 'learn_rate':0.1, 'epochs':1, 'denoise_type':0, \n",
    "                                                'batch_size':10, 'shuffle_batches':False, 'usevola':False } ):\n",
    "        \n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tqdm.tqdm(total=len(datatorun)) as pbar:\n",
    "            # We create a TensorFlow session to use the graph\n",
    "            sess = tf.Session()\n",
    "\n",
    "            stocks_universe = datatorun.items\n",
    "            N = len(stocks_universe)\n",
    "            #cov_diffs = []\n",
    "\n",
    "\n",
    "            def initialize(context):\n",
    "                # Set-up system parameters\n",
    "                context.rebalance_every_nthbar = rebalfreq\n",
    "                context.covlen = covlen\n",
    "                context.trainlen = trainlen\n",
    "                context.datalen = max(context.rebalance_every_nthbar,context.trainlen) + context.covlen\n",
    "                context.layerdims = args_ae['layer_dims']\n",
    "\n",
    "                # Register history container to keep a window of past prices.\n",
    "                add_history(context.datalen, '1d', 'price')\n",
    "                # Turn off the slippage model and set some realistic commissions for backtester\n",
    "                set_slippage(slippage.FixedSlippage(spread=0.0))\n",
    "                set_commission(commission.PerShare(cost=0.01, min_trade_cost=1.0))\n",
    "                context.tick = 0\n",
    "\n",
    "                # Create AutoEncoder graph\n",
    "                dims = [N*(N-1)/2+N]\n",
    "                dims.extend(context.layerdims)\n",
    "                ae = autoencoder(dimensions=dims)\n",
    "                optimizer = tf.train.AdamOptimizer(args_ae['learn_rate']).minimize(ae['cost'])\n",
    "\n",
    "                # Define and log-writer for TensorBoard\n",
    "                writer = tf.train.SummaryWriter(\"./tb_logs/logs\", sess.graph_def)\n",
    "                merged = tf.merge_all_summaries()\n",
    "                # Initialize all TF variables\n",
    "                sess.run(tf.initialize_all_variables())\n",
    "\n",
    "                # Finalizing helps us to make sure that we don't add to the graph later by accident\n",
    "                g.finalize()\n",
    "                tensorflowcontext = {'sess':sess,'ae':ae, 'optimizer':optimizer, 'writer':writer, 'merged':merged}\n",
    "                context.tfcontext = tensorflowcontext\n",
    "\n",
    "\n",
    "\n",
    "            def handle_data(context, data):\n",
    "                # Allow history to accumulate enough prices before trading\n",
    "                # and rebalance every n-day thereafter.\n",
    "                context.tick += 1\n",
    "                if context.tick < context.datalen:\n",
    "                    return\n",
    "\n",
    "                if context.tick % context.rebalance_every_nthbar == 0:   # rebalance every nth bar\n",
    "                    pbar.update(context.tick)\n",
    "                    #print '.',     \n",
    "\n",
    "                    # Get rolling window of past prices and compute returns\n",
    "                    prices = history(context.datalen, '1d', 'price').dropna()\n",
    "                    returns = prices.pct_change().dropna()\n",
    "                    datalen = len(returns)        \n",
    "                    samplecovs = [ np.asarray(returns[i:i+context.covlen].cov()) \n",
    "                                  for i in range (datalen-context.covlen+1) ]\n",
    "\n",
    "                    covsample = samplecovs[-1]\n",
    "                    \n",
    "                    assert corrupted_inp>=0 and corrupted_inp<=1\n",
    "                    for i in xrange(5):\n",
    "                        if(np.random.random() < corrupted_inp):        \n",
    "                            corr, vola = cov2corrvola(covsample)\n",
    "                            # corrupt a single correlation relationship randomly \n",
    "                            corrcrpt = np.copy(corr)                        \n",
    "                            idx1 = np.random.randint(N)\n",
    "                            idx2 = idx1\n",
    "                            while idx1==idx2:\n",
    "                                idx2 = np.random.randint(N)\n",
    "                            corrcrpt[idx1,idx2] = np.min([1.0,np.max([-1,-3 * corrcrpt[idx1,idx2]])])\n",
    "                            corrcrpt[idx2,idx1] = corrcrpt[idx1,idx2]\n",
    "                            covsample = corr2cov(nearcorr(corrcrpt),vola)\n",
    "\n",
    "\n",
    "                    covmats2dump = []\n",
    "                    weights2dump = []\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        # Min-Variance portfolio optimization\n",
    "                        weights = optimal_portfolio_cvxopt(covsample) \n",
    "                        if dump_covs:\n",
    "                            covmats2dump.append(covsample)            \n",
    "                            weights2dump.append(weights)\n",
    "\n",
    "                        if use_autoencoder:\n",
    "                            covshrunk = np.copy(covsample)\n",
    "                            if( len(samplecovs)>=args_ae['batch_size'] ):\n",
    "                                covshrunk = DeepShrinkCovmat(samplecovs, tensorflowcontext=context.tfcontext, \n",
    "                                                             covmat_target=covsample,  args_ae=args_ae)\n",
    "                            weights_sh = optimal_portfolio_cvxopt(covshrunk)\n",
    "                            weights = weights_sh\n",
    "                            #cov_diffs.append(np.sqrt(np.mean(np.square(y-x))))\n",
    "                            if dump_covs:\n",
    "                                covmats2dump.append(covshrunk)          \n",
    "                                weights2dump.append(weights)\n",
    "                                ShowCovarsWeights(covmats2dump, weights2dump, stocks_universe, \n",
    "                                        './img/covwei_' + get_datetime().strftime(\"%Y%m%d\") + \".png\")\n",
    "\n",
    "\n",
    "                        # Rebalance portfolio accordingly\n",
    "                        for stock, weight in zip(prices.columns, weights):\n",
    "                            order_target_percent(stock, weight)\n",
    "                    except ValueError as e:\n",
    "                        print 'exception in optim or shrinkage: ' + str(e)\n",
    "\n",
    "\n",
    "        # Instantinate algorithm        \n",
    "        algo = TradingAlgorithm(initialize=initialize, handle_data=handle_data)\n",
    "        \n",
    "        # Run algorithm\n",
    "        start_time = time.time()\n",
    "        results = algo.run(datatorun)\n",
    "        runtime = time.time() - start_time\n",
    "        print(\"\\nRuntime: {:.1f} sec, Sharpe: {:.2f}\".format(runtime,\n",
    "                                                            results.sharpe[-1]))\n",
    "        return results#, cov_diffs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the backtests look like?\n",
    "\n",
    "Let's run the backtest with and without autoencoder shrinkage and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rebal_freq = 20\n",
    "cov_len = 250\n",
    "train_len = 20\n",
    "offset = 2*max(train_len,rebal_freq)+cov_len+1\n",
    "\n",
    "sim_args = { 'layer_dims':[48,24,4], 'learn_rate':0.001, 'epochs':1, 'denoise_type':1, \n",
    "             'batch_size':10, 'shuffle_batches':True, 'usevola':False } \n",
    "\n",
    "res1 = RunBacktest(data, rebalfreq=rebal_freq, covlen=cov_len, trainlen = train_len,\n",
    "                          use_autoencoder=False)\n",
    "\n",
    "res2 = RunBacktest(data, rebalfreq=rebal_freq, covlen=cov_len, trainlen = train_len,\n",
    "                         use_autoencoder=True, args_ae = sim_args)              \n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "res1.ending_value[offset:].plot(label='sample covariance',legend=True)\n",
    "res2.ending_value[offset:].plot(label='autoencoder',legend=True, title='Equity Curve');\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "res1.sharpe[offset:].plot(label='sample covariance',legend=True)\n",
    "res2.sharpe[offset:].plot(label='autoencoder',legend=True, title='Period Sharpe');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results looks reasonable and regularization method seem to be contributing in the above example. Obviously, we need to run many more simulations in order to make any conclusion. The example above is for demonstration purposes only and the real power of such an approach will be arguably only demonstrable with bigger universes and a lot more data (i.e. higher frequencies).\n",
    "\n",
    "### Another use case for AutoEncoder Regularization:\n",
    "Assume we have an upcoming datastream and we are estimating our covariance matrix and changing underlying portfolio accordingly in a continous fashion throughtout the day. What happens if we receive a bad data point? Autoencoders are useful fixing out those problematic cases. Below we will deliberately corrupt one point in our covariance matrix and show that by simply filtering it throughout the AutoEncoder network we will get the cleaned up version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = data[:,-700:,'price']\n",
    "returns = dat.pct_change().dropna()\n",
    "datalen = len(returns)\n",
    "covlen = 250\n",
    "\n",
    "training_covs = [ np.asarray(returns[i:i+covlen].cov()) for i in range(datalen-covlen+1) ]\n",
    "cov_target = training_covs[-1]\n",
    "\n",
    "\n",
    "sim_args = { 'layer_dims':[64,32,4], 'learn_rate':0.001, 'epochs':2, 'denoise_type':1, \n",
    "             'batch_size':10, 'shuffle_batches':True, 'usevola':False } \n",
    "\n",
    "\n",
    "cov_target_corrupted = np.copy(cov_target)\n",
    "cov_target_corrupted[0,1] = cov_target_corrupted[1,0] = -10 * cov_target_corrupted[0,1]\n",
    "\n",
    "ShowCovarianceMatrix(cov_target_corrupted, universe, title='Corrupted')\n",
    "\n",
    "cov_target_fixed = DeepShrinkCovmat(training_covs, covmat_target=cov_target_corrupted, \n",
    "                                    args_ae = sim_args )\n",
    "\n",
    "ShowCovarianceMatrix(cov_target_fixed, universe, title='Fixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also try a similar approach in a simulation setting where we corrupt the covariance matrix with some probability at every observation and track how such an environment would effect our backtest. By using such a regularization technique, we not only have a more robust and stable system, by simply tracking the cost value of the AutoEncoder we will have a Anomaly detection capabilities. Anytime the cost is too high, that tells us something suspicious is happening (either bad data or significant market change) which may require some closer attention. We don't care about the performance below that much, it can be pretty much random. In TensorBoard, we can see how the average cost higher than previous case is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rebal_freq = 20\n",
    "cov_len = 250\n",
    "train_len = 20\n",
    "offset = 2*max(train_len,rebal_freq)+cov_len+1\n",
    "\n",
    "sim_args = { 'layer_dims':[48,24,4], 'learn_rate':0.001, 'epochs':1, 'denoise_type':1, \n",
    "             'batch_size':10, 'shuffle_batches':True, 'usevola':False } \n",
    "\n",
    "\n",
    "res1 = RunBacktest(data, rebalfreq=rebal_freq, covlen=cov_len, trainlen = train_len,\n",
    "                          use_autoencoder=False,corrupted_inp=0.7)\n",
    "\n",
    "res2 = RunBacktest(data, rebalfreq=rebal_freq, covlen=cov_len, trainlen = train_len,\n",
    "                         use_autoencoder=True, args_ae = sim_args, corrupted_inp=0.7)              \n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "res1.ending_value[offset:].plot(label='sample covariance',legend=True)\n",
    "res2.ending_value[offset:].plot(label='autoencoder',legend=True, title='Equity Curve');\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "res1.sharpe[offset:].plot(label='sample covariance',legend=True)\n",
    "res2.sharpe[offset:].plot(label='autoencoder',legend=True, title='Period Sharpe');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Prepared for QuantCon 2016 \n",
    "@ Erk Subasi, 2016, Limmat Capital Alternative Investments AG\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
